# Perses Observability Dashboard Integration - UX Design Document

**Project**: OpenShift AI Dashboard - Perses Integration
**Document Type**: User Experience & Architecture Design
**Author**: Aria (UX Architect)
**Date**: 2025-10-16

---

## Executive Summary

This document defines the user experience strategy for integrating Perses observability dashboards into the OpenShift AI Dashboard. The design prioritizes user-centered workflows, ecosystem coherence, and operational accessibility across three primary personas: Data Scientists, MLOps Engineers, and OpenShift AI Administrators.

**Key Design Principles:**
- Journey-focused: Support complete diagnostic and monitoring workflows
- Ecosystem-aware: Integrate seamlessly with existing ODH Dashboard navigation and mental models
- Role-appropriate: Provide persona-specific views while maintaining consistency
- Accessible: WCAG 2.1 AA compliant for equitable access
- Performant: Responsive experience even with large-scale deployments

---

## 1. PRIMARY USE CASES

### Use Case 1: Monitor Model Serving Performance in Production

**Primary Actor:** MLOps Engineer

**Description:** Monitor health, latency, throughput, and error rates of deployed models to ensure SLA compliance and identify performance degradation.

**Main Success Scenario:**
1. Navigate to "Observability" â†’ "Model Serving"
2. View default "Model Serving Overview" with all accessible models
3. Filter to specific project/namespace
4. Review real-time metrics: latency (p50, p95, p99), throughput, errors, resources
5. Identify elevated p99 latency on model endpoint
6. Drill down to model-specific dashboard
7. Analyze detailed metrics and traces for root cause
8. Take remediation action (adjust resources, redeploy)

**Alternative Flows:**
- No metrics available â†’ Show guided setup
- Access restricted â†’ Show filtered view with messaging
- Historical analysis â†’ Switch time range to days/weeks
- Alert investigation â†’ Deep link from alert notification

---

### Use Case 2: Debug Data Science Pipeline Failures

**Primary Actor:** Data Scientist

**Description:** Understand why pipeline failed or performed slowly by examining execution metrics, step durations, and resource constraints.

**Main Success Scenario:**
1. Receive pipeline failure notification
2. Navigate to "Observability" from pipeline run details (contextual link)
3. Dashboard loads pre-filtered to specific pipeline run ID
4. View pipeline topology with execution times per step
5. Identify bottleneck: preprocessing step 3x slower than expected
6. Examine memory usage showing OOM events
7. Return to pipeline definition to increase memory allocation
8. Re-run pipeline with adjustments

**Alternative Flows:**
- Comparative analysis â†’ Side-by-side successful vs. failed runs
- Resource optimization â†’ Right-size requests for cost efficiency
- Data quality issues â†’ Trace upstream data problems
- First-time user â†’ Guided tour of key metrics

---

### Use Case 3: Establish Observability Standards Across Platform

**Primary Actor:** OpenShift AI Administrator

**Description:** Create, manage, and distribute standardized dashboards across teams for consistent monitoring, compliance, and operational excellence.

**Main Success Scenario:**
1. Navigate to "Observability Management" (admin section)
2. View catalog of dashboard templates
3. Create new dashboard using Perses YAML (Dashboard-as-Code)
4. Configure distribution: assign to namespaces/roles
5. Set permissions (view-only vs. edit)
6. Save to Git repository (GitOps enabled)
7. Dashboard auto-deploys to target users
8. Monitor usage analytics for adoption tracking

**Alternative Flows:**
- Import community dashboards from public repos
- Update existing dashboard â†’ Users see migration notification
- Emergency diagnostic dashboard during incident
- Compliance audit â†’ Generate coverage reports

---

### Use Case 4: Investigate Model Drift and Data Quality

**Primary Actor:** MLOps Engineer

**Description:** Monitor production models for data drift, prediction shifts, and input quality issues impacting accuracy over time.

**Main Success Scenario:**
1. Access "Model Monitoring" dashboard
2. View prediction distribution metrics
3. Notice statistical drift alert on fraud detection model
4. Drill into model-specific dashboard
5. Compare current vs. training baseline feature distributions
6. Identify transaction amount distribution shift
7. Correlate with Tempo traces for upstream pipeline changes
8. Trigger retraining with recent production data

**Alternative Flows:**
- False positive â†’ Seasonal pattern, not drift
- Multi-model comparison â†’ Champion/challenger variants
- Gradual degradation â†’ Set custom drift alert thresholds
- Root cause in data pipeline â†’ Trace transformation bug

---

### Use Case 5: Optimize Notebook Resource Usage

**Primary Actor:** OpenShift AI Administrator

**Description:** Visibility into Jupyter notebook utilization to optimize capacity, identify waste, and enforce governance.

**Main Success Scenario:**
1. Open "Workbenches & Notebooks" dashboard
2. View aggregate metrics: active notebooks, idle time, CPU/memory/GPU usage
3. Filter by project for team-specific patterns
4. Identify notebooks idle >24hr with GPU resources
5. Drill down to specific users and instances
6. Refine auto-culling policies and quotas
7. Share utilization reports with team leads
8. Track improvements after policy changes

**Alternative Flows:**
- Capacity planning â†’ Forecast future needs
- Cost allocation â†’ Export for chargeback/showback
- Performance optimization â†’ Identify under-provisioned notebooks
- Compliance verification â†’ No policy violations

---

## 2. USER WORKFLOWS

### Workflow A: Discovering and Accessing Observability Dashboards

**Entry Points:**
1. **Primary Navigation**: Top-level "Observability" in left sidebar (after Model Serving, before Settings)
2. **Contextual Links**: From Model Serving cards, Pipeline runs, Data Connections
3. **Dashboard Home**: Quick access to recent/favorited dashboards
4. **Global Search**: Keywords like "observability", "metrics", "monitoring"

**Discovery Journey:**
```
Awareness â†’ Exploration â†’ Context â†’ Access â†’ Orientation â†’ Action
```

**Information Architecture:**
```
Observability (Main Section)
â”œâ”€â”€ Overview (Landing page)
â”‚   â”œâ”€â”€ Quick Stats
â”‚   â”œâ”€â”€ System Health
â”‚   â””â”€â”€ Recent Alerts
â”œâ”€â”€ Model Serving
â”‚   â”œâ”€â”€ All Models Overview
â”‚   â”œâ”€â”€ Individual Model Details
â”‚   â””â”€â”€ KServe Infrastructure
â”œâ”€â”€ Pipelines
â”‚   â”œâ”€â”€ Pipeline Runs Overview
â”‚   â”œâ”€â”€ Performance Analytics
â”‚   â””â”€â”€ Individual Pipeline Details
â”œâ”€â”€ Workbenches & Notebooks
â”‚   â”œâ”€â”€ Resource Utilization
â”‚   â”œâ”€â”€ Active Sessions
â”‚   â””â”€â”€ User Activity
â”œâ”€â”€ Infrastructure (admin)
â”‚   â”œâ”€â”€ Cluster Resources
â”‚   â”œâ”€â”€ Storage Performance
â”‚   â””â”€â”€ Network Traffic
â””â”€â”€ Custom Dashboards
    â””â”€â”€ [User-defined]
```

---

### Workflow B: Viewing Metrics for AI Workloads

**Loading Experience:**
- Progressive loading: Critical metrics < 2s, detailed panels follow
- Skeleton loaders show structure during load
- Partial data display if some metrics unavailable

**Understanding Metrics:**
- Visual hierarchy: Health/latency/errors top-left
- Consistent styling across dashboards
- Inline tooltips explain metric definitions
- Color-coded thresholds: green (healthy), yellow (warning), red (critical)

**Interactions:**
- Time range: Last 15min, 1hr, 6hr, 24hr, 7d, 30d, custom
- Auto-refresh: 5s, 15s, 30s, 1min intervals with pause
- Zoom/pan: Click-drag to zoom, reset button
- Data inspection: Hover for values, click for details

**Contextual Actions:**
- Export dashboard (PNG/PDF)
- Share URL with filters
- Set alert rules (future)
- Navigate to related resources

---

### Workflow C: Filtering & Scoping Dashboards

**Filter Hierarchy:**

**Level 1 - Namespace/Project (Primary):**
- Dropdown: "All Projects" or specific selection
- Respects Kubernetes RBAC
- Persists across navigation
- Visual indicator of active filter

**Level 2 - Workload Type:**
- Tabs/chips: "All", "Model Serving", "Pipelines", "Notebooks"

**Level 3 - Resource-Specific:**
- Model name, pipeline ID, user, pod
- Auto-complete with search
- Multiple selection support

**Level 4 - Metric-Specific:**
- Prometheus label filters
- Advanced mode for power users

**UX Features:**
- Persistent filters (browser local storage)
- Filter templates: "My Active Models", "Production Only"
- Clear all button
- Breadcrumb chips for active filters
- Deep linking: URL encodes filters

---

### Workflow D: Troubleshooting Performance Issues

**Diagnostic Journey:**

**Phase 1 - Detection:**
- Notice issue (slow response, timeout, alert)
- Navigate from affected resource or alert link
- Dashboard loads with incident time context

**Phase 2 - Scoping:**
- Overview shows high-level indicators
- Identify affected components via visual anomalies
- Narrow time range to incident window

**Phase 3 - Investigation:**
- Drill down to component dashboard
- Multi-panel correlated view:
  - Performance (latency, throughput)
  - Resources (CPU, memory, GPU)
  - Errors (HTTP 5xx, exceptions)
  - Dependencies (database, storage, APIs)
- Compare with baseline

**Phase 4 - Root Cause:**
- Identify metric pattern (e.g., memory exhaustion)
- Switch to trace view (Tempo integration)
- Examine log correlation
- Form hypothesis

**Phase 5 - Validation:**
- Make configuration change
- Monitor dashboard for improvement
- Validate metrics return to healthy
- Document findings (annotations, runbooks)

**UX Support:**
- Correlation highlighting during same period
- Comparison mode: good vs. bad periods
- Timeline annotations for deployments/changes
- Suggested related dashboards
- Guided checklists for common issues

---

## 3. NAVIGATION & INFORMATION ARCHITECTURE

### ODH Dashboard Integration

**Placement:**
```
OpenShift AI Dashboard
â”œâ”€â”€ Home / Overview
â”œâ”€â”€ Applications
â”œâ”€â”€ Data Science Projects
â”œâ”€â”€ Model Serving
â”œâ”€â”€ Data Science Pipelines
â”œâ”€â”€ [NEW] Observability â† Top-level item
â”‚   â”œâ”€â”€ Overview
â”‚   â”œâ”€â”€ Model Serving
â”‚   â”œâ”€â”€ Pipelines
â”‚   â”œâ”€â”€ Workbenches
â”‚   â”œâ”€â”€ Infrastructure (admin)
â”‚   â””â”€â”€ Custom Dashboards
â”œâ”€â”€ Resources
â”œâ”€â”€ Settings
â””â”€â”€ [User Profile]
```

**Contextual Integration:**

1. **Model Serving Page:**
   - Mini-metrics preview on model cards (sparklines)
   - "View Metrics" button â†’ filtered observability
   - Health status indicator (green/yellow/red)

2. **Pipeline Runs:**
   - Run list shows duration, status, resource summary
   - Details page: "Performance Metrics" tab with embedded dashboard

3. **Data Science Project:**
   - Project overview: "Observability" tab
   - Project-scoped aggregate metrics

4. **Notebook Launch:**
   - Active notebook list: real-time resource bars
   - "View Metrics" â†’ notebook-specific dashboard

5. **Global Alerts:**
   - Header notification icon with count
   - Click â†’ alert summary â†’ dashboard links

**Navigation Behavior:**
- Breadcrumbs: `Home > Observability > Model Serving > [Model]`
- Browser back returns to previous context
- Deep linking: `/observability/model-serving?project=fraud&model=rf-v2`
- State preservation during session

---

## 4. PERSONA-SPECIFIC NEEDS

### Data Scientists

**Context:** Experimentation, model development, pipeline iteration

**Dashboard Priorities:**
1. My Experiments Dashboard
   - Pipeline run durations, step breakdown
   - Training metrics (loss, accuracy)
   - Resource utilization
   - Cost estimation

2. Notebook Performance
   - Personal resource usage
   - Kernel restart frequency
   - Library load times

3. Model Training Insights
   - GPU utilization
   - Data loading bottlenecks
   - Hyperparameter comparison

**UX Adaptations:**
- Simplified language (avoid Kubernetes jargon)
- Guided interpretation and recommendations
- Comparative experiment views
- Default filter to "my resources"
- Learning resource links

**Example Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ My Recent Pipeline Runs (Last 7 Days)  â”‚
â”‚ [Timeline: success/failure, durations]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Current Run     â”‚ â”‚ Resource Usage   â”‚
â”‚ Step 3/5        â”‚ â”‚ CPU: 85%         â”‚
â”‚ Training 23min  â”‚ â”‚ Mem: 12GB/16GB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### MLOps Engineers

**Context:** Production reliability, performance optimization, SLO compliance

**Dashboard Priorities:**
1. Production Model Health
   - SLI/SLO tracking
   - RED method (Rate, Errors, Duration)
   - Model drift detection
   - Dependency health

2. Infrastructure Performance
   - KServe/ModelMesh resources
   - Autoscaling behavior
   - Resource saturation
   - Network performance

3. Pipeline Orchestration
   - Success rates, retries
   - Queue depths, parallelism
   - Data volume trends

4. Cost & Efficiency
   - Resource costs by project
   - Utilization vs. requested
   - Idle time analysis

**UX Adaptations:**
- Advanced PromQL query builder
- Multi-model comparison views
- Alert integration and creation
- Trace correlation (Tempo)
- GitOps export (YAML)
- API access links
- Runbook integration

**Example Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Uptime   â”‚ Errors   â”‚ P99 Lat  â”‚ Req/min  â”‚
â”‚ 99.97%   â”‚ 0.03%    â”‚ 234ms    â”‚ 15,234   â”‚
â”‚ ğŸŸ¢ 99.95%â”‚ ğŸŸ¢ <0.1% â”‚ ğŸŸ¡ <200msâ”‚ [Trend]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Latency Heatmap (Last 1 hour)              â”‚
â”‚ [Time x Latency bucket visualization]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### OpenShift AI Administrators

**Context:** Platform health, capacity management, governance, enablement

**Dashboard Priorities:**
1. Platform Overview
   - Cluster health (CPU, memory, GPU, storage)
   - User activity, concurrent workloads
   - Service health (ODH components)
   - Incidents, alerts, anomalies

2. Multi-Tenancy & RBAC
   - Resource usage by project/team
   - Quota utilization
   - Permission audit trail
   - Orphaned resources

3. Capacity Planning
   - Historical trends (3mo, 6mo, 1yr)
   - Growth projections
   - Resource efficiency scores
   - Cost forecasts

4. Dashboard Governance
   - Dashboard catalog
   - Usage statistics
   - Version control history
   - Orphaned dashboards

**UX Adaptations:**
- Executive summary views
- Drill-down hierarchy (platform â†’ project â†’ workload)
- Bulk operations (multi-select)
- Audit trail visibility
- Template creation tools
- Export/reporting (PDF)

**Admin-Only Features:**
- Dashboard CRUD UI
- Access control editor
- Monitoring stack health
- Global settings

**Example Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Platform Health Summary                    â”‚
â”‚ [Service indicators: Green/Yellow/Red]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workloads   â”‚ â”‚ Cluster Capacity           â”‚
â”‚ Models: 23  â”‚ â”‚ CPU: 67% [â– â– â– â– â– â– â– â–¡â–¡â–¡]     â”‚
â”‚ Pipelines:12â”‚ â”‚ Mem: 72% [â– â– â– â– â– â– â– â–¡â–¡â–¡]     â”‚
â”‚ Notebooks:45â”‚ â”‚ GPU: 89% [â– â– â– â– â– â– â– â– â– â–¡] âš ï¸  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. CUSTOMER CONSIDERATIONS

### Multi-Tenancy

**Design Approach:**

**1. Namespace-Based Isolation:**
- Metric queries auto-scoped to accessible namespaces
- Perses dashboards use dynamic variables: `namespace=~"$user_namespaces"`
- Backend API proxy injects namespace filter from Kubernetes permissions
- Transparent to users

**2. Dashboard Distribution Models:**

**Platform Dashboards (Admin-Provided):**
- Pre-built for common use cases
- Deployed as CRDs in centralized namespace
- Accessible to all, data auto-filtered
- Read-only for standard users

**Project Dashboards (Team-Specific):**
- Teams create in their project namespace
- Visible only to team members
- Stored as namespace-scoped CRDs

**Personal Dashboards (User-Created):**
- Individual custom dashboards
- User-specific namespace or ConfigMap
- Opt-in sharing

**3. Cross-Namespace Aggregation:**
- Admin dashboards use `namespace=""` or regex
- Requires elevated permissions
- Clear UI indicator: "Viewing platform-wide data"

---

### Role-Based Access Control (RBAC)

**Permission Model:**
```yaml
User: alice@example.com
Kubernetes Groups: [fraud-detection-team, data-scientists]
Namespace Access:
  - fraud-detection (edit)
  - shared-datasets (view)
Observability Permissions:
  - View dashboards: fraud-detection, shared-datasets
  - Edit dashboards: fraud-detection
  - Cannot view: other-team-project
```

**RBAC Levels:**

**Dashboard View:**
- Viewer: View, change time/filters (no save)
- Editor: Modify layout, add panels, save
- Admin: Create/delete, manage distribution

**Metric Data:**
- Prometheus/Tempo RBAC proxies
- JWT token validation
- Queries filtered by namespace permissions

**Dashboard Management:**
- CRD-based: Standard Kubernetes verbs
- Edit permission on namespace â†’ create dashboards
- Cluster-level for platform dashboards

**UX:**
- Lock icons for read-only dashboards
- Friendly permission error messages
- "Create a copy to customize" for platform dashboards
- Contextual help links

---

### Accessibility (WCAG 2.1 AA)

**Visual Accessibility:**
- Color-blind safe palettes (never color alone)
- Contrast ratios: Text 4.5:1, UI 3:1
- Configurable font sizes, zoomable to 200%
- Dark mode support (optional)

**Keyboard Navigation:**
- Full tab support, logical order
- Skip links, focus indicators (2px outline)
- Shortcuts: R (refresh), T (time range), F (filter), ? (help), Esc (close)

**Screen Reader Support:**
- Semantic HTML, ARIA landmarks/roles
- Chart alternative text descriptions
- Data table alternatives to visuals
- Live regions: `aria-live="polite"` for updates

**Content:**
- Clear language, defined jargon
- Descriptive error messages
- Human-readable metric labels
- Internationalization support

**Temporal Content:**
- Auto-refresh controls (pause, interval)
- Announce major changes
- Disable animations option

**Testing:**
- Automated: aXe, WAVE, Lighthouse (CI/CD)
- Manual: Keyboard-only, screen readers (NVDA, JAWS, VoiceOver, Orca)
- User testing with people with disabilities

---

### Performance with Large Datasets

**Targets:**
- Initial load: < 3s (p95)
- Query response: < 2s (p95)
- Interactions: < 500ms
- Auto-refresh: < 200ms incremental

**Optimization Strategies:**

**1. Query Optimization:**
- Efficient PromQL (avoid high cardinality)
- Recording rules for common aggregations
- Pre-computed dashboards query recording rules

**2. Data Sampling:**
- Downsampling for long time ranges (>24hr â†’ 5min resolution)
- Configurable, user-requestable high resolution
- Transparent UI indicator

**3. Progressive Loading:**
```
Sequence:
1. Critical KPIs (simple queries)
2. Primary charts (moderate complexity)
3. Detail tables/heatmaps (high cardinality)
4. Optional panels (on expand)
```
- Lazy loading below fold
- Collapsed groups load on expand

**4. Caching:**
- Browser: Static definitions, query results (15-60s TTL)
- Backend: API gateway query cache, dashboard definitions

**5. UI Performance:**
- Virtual scrolling for long lists
- Canvas rendering for dense charts
- Debounced inputs (300ms)
- Disabled animations by default

**6. Scalability Patterns:**
- Dashboard tiers: Quick, Standard, Detailed
- Pagination (20-50 items/page)
- Infinite scroll for exploration

**7. User Feedback:**
- Skeleton loaders, progress indicators
- Cancel button for slow queries
- Performance warnings for wide ranges
- Reduced data mode preference

---

### Additional Considerations

**Data Retention:**
- Metrics: 15-90 days (platform policy)
- Aggregated: Longer via recording rules
- Compliance: GDPR, HIPAA for user activity
- Export: Download for archival

**Network:**
- Offline mode (cached definitions, read-only)
- Low bandwidth mode (reduced resolution, text tables)
- Tested at 100ms, 250ms, 500ms latency

**Customization:**
- Branding support (logo, colors)
- Perses plugin system (custom panels, sources)
- REST API for programmatic management
- Embeddable components (iframes)

**Documentation:**
- Role-specific tutorials
- Dashboard template gallery
- Troubleshooting guide
- Community links

**Migration:**
- Grafana import (if applicable)
- First-time user wizard
- Demo environment with synthetic data

**Security:**
- OpenShift OAuth integration
- Kubernetes RBAC authorization
- Audit logging (access, modifications)
- Secrets management (Kubernetes Secrets)
- Network policies (restrict backend)

---

## APPENDIX: JOURNEY MAPS

### Data Scientist Journey: Pipeline Troubleshooting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Awareness   â”‚ Investigationâ”‚ Understandingâ”‚ Resolution   â”‚ Validation  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Pipeline    â”‚ Click alert  â”‚ View metrics â”‚ Adjust       â”‚ Monitor     â”‚
â”‚ fails,      â”‚ link â†’       â”‚ â†’ identify   â”‚ memory       â”‚ dashboard   â”‚
â”‚ receive     â”‚ Observabilityâ”‚ OOM issue    â”‚ allocation   â”‚ â†’ confirm   â”‚
â”‚ notificationâ”‚              â”‚              â”‚              â”‚ success     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Feeling:    â”‚ Feeling:     â”‚ Feeling:     â”‚ Feeling:     â”‚ Feeling:    â”‚
â”‚ Frustrated  â”‚ Curious      â”‚ Enlightened  â”‚ Confident    â”‚ Relieved    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Touchpoint: â”‚ Touchpoint:  â”‚ Touchpoint:  â”‚ Touchpoint:  â”‚ Touchpoint: â”‚
â”‚ Email/Slack â”‚ ODH Dashboardâ”‚ Perses       â”‚ Pipeline     â”‚ Perses      â”‚
â”‚             â”‚              â”‚ Dashboard    â”‚ Editor       â”‚ Dashboard   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MLOps Engineer Journey: Production Monitoring

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Routine     â”‚ Detection    â”‚ Diagnosis    â”‚ Mitigation   â”‚ Post-mortem â”‚
â”‚ Check       â”‚              â”‚              â”‚              â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Open daily  â”‚ Notice p99   â”‚ Drill into   â”‚ Scale up     â”‚ Annotate    â”‚
â”‚ health      â”‚ latency spikeâ”‚ model        â”‚ replicas,    â”‚ timeline,   â”‚
â”‚ dashboard   â”‚ on overview  â”‚ dashboard    â”‚ adjust       â”‚ document    â”‚
â”‚             â”‚              â”‚              â”‚ resources    â”‚ in runbook  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Feeling:    â”‚ Feeling:     â”‚ Feeling:     â”‚ Feeling:     â”‚ Feeling:    â”‚
â”‚ Calm        â”‚ Alert        â”‚ Focused      â”‚ Determined   â”‚ Satisfied   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Touchpoint: â”‚ Touchpoint:  â”‚ Touchpoint:  â”‚ Touchpoint:  â”‚ Touchpoint: â”‚
â”‚ Perses      â”‚ Perses       â”‚ Perses +     â”‚ kubectl +    â”‚ Perses      â”‚
â”‚ Overview    â”‚ Dashboard    â”‚ Tempo traces â”‚ ODH UI       â”‚ Annotations â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Administrator Journey: Dashboard Governance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Planning    â”‚ Creation     â”‚ Distribution â”‚ Monitoring   â”‚ Iteration   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Identify    â”‚ Design       â”‚ Apply to     â”‚ Track        â”‚ Gather      â”‚
â”‚ team need   â”‚ dashboard    â”‚ target       â”‚ adoption     â”‚ feedback,   â”‚
â”‚ for model   â”‚ using YAML,  â”‚ namespaces,  â”‚ metrics,     â”‚ refine      â”‚
â”‚ monitoring  â”‚ test locally â”‚ commit to Gitâ”‚ user feedbackâ”‚ dashboard   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Feeling:    â”‚ Feeling:     â”‚ Feeling:     â”‚ Feeling:     â”‚ Feeling:    â”‚
â”‚ Strategic   â”‚ Creative     â”‚ Efficient    â”‚ Informed     â”‚ Responsive  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Touchpoint: â”‚ Touchpoint:  â”‚ Touchpoint:  â”‚ Touchpoint:  â”‚ Touchpoint: â”‚
â”‚ Stakeholder â”‚ IDE + Perses â”‚ GitOps +     â”‚ Admin        â”‚ Perses +    â”‚
â”‚ meetings    â”‚ validator    â”‚ kubectl      â”‚ Dashboard    â”‚ Git PR      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## DESIGN ARTIFACTS NEEDED (FUTURE WORK)

**High-Fidelity Mockups:**
1. Observability section landing page
2. Model Serving dashboard (overview + detail)
3. Pipeline performance dashboard
4. Notebook resource utilization dashboard
5. Admin dashboard management UI
6. Mobile/responsive layouts

**Prototypes:**
1. Interactive dashboard navigation flow
2. Filter and time range selection interactions
3. Drill-down and comparison workflows
4. Alert integration prototype

**User Research:**
1. Persona interviews (validate assumptions)
2. Card sorting (validate information architecture)
3. Usability testing (validate workflows)
4. Accessibility testing (validate WCAG compliance)

**Design System Assets:**
1. Perses component library integration with PatternFly
2. Chart color palettes (accessible)
3. Loading states and skeletons
4. Error states and empty states
5. Icon set for metrics and status indicators

---

## SUCCESS METRICS

**Usage Metrics:**
- Observability section adoption rate (% active users)
- Dashboard views per user per week
- Most-used dashboards (feature validation)
- Filter usage patterns (understand user needs)

**Performance Metrics:**
- Dashboard load time (p50, p95)
- Query response time (p50, p95)
- Error rate (failed queries, timeouts)
- User-reported performance issues

**User Satisfaction:**
- NPS (Net Promoter Score) for observability feature
- Task completion rates (troubleshooting scenarios)
- Time to resolution for common issues
- Support ticket volume (observability-related)

**Business Impact:**
- Reduction in MTTR (Mean Time To Resolution)
- Increased model uptime and SLA compliance
- Resource optimization (cost savings)
- Platform adoption (user enablement)

---

## RISKS & MITIGATION

**Risk: Performance degradation with high metric cardinality**
- Mitigation: Recording rules, query optimization, downsampling, user education

**Risk: Users don't discover observability features**
- Mitigation: Contextual links, onboarding wizard, in-app hints, documentation

**Risk: RBAC misconfiguration leads to unauthorized data access**
- Mitigation: Security audit, automated testing, clear admin docs

**Risk: Accessibility gaps remain unaddressed**
- Mitigation: Automated testing in CI/CD, manual testing protocols, user feedback

**Risk: Dashboard maintenance burden for admins**
- Mitigation: Template library, GitOps automation, usage analytics to identify unused dashboards

---

## REFERENCES

**Perses Documentation:**
- https://perses.dev/
- Perses Dashboard CRD specification
- Perses plugin system

**Design Standards:**
- WCAG 2.1 Guidelines: https://www.w3.org/WAI/WCAG21/quickref/
- PatternFly Design System: https://www.patternfly.org/
- Material Design Accessibility: https://material.io/design/usability/accessibility.html

**Observability Best Practices:**
- Google SRE Book: https://sre.google/books/
- RED Method (Request rate, Error rate, Duration)
- USE Method (Utilization, Saturation, Errors)

**OpenShift AI Context:**
- ODH Dashboard: https://github.com/opendatahub-io/odh-dashboard
- KServe: https://kserve.github.io/website/
- Kubeflow Pipelines: https://www.kubeflow.org/docs/components/pipelines/

---

**End of Document**
