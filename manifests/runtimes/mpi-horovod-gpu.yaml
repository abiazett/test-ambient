# ClusterTrainingRuntime for Horovod with GPU Support
#
# This runtime template provides a production-ready configuration for distributed
# training using Horovod framework with NVIDIA GPUs.
#
# Use Case: Deep learning training with TensorFlow or PyTorch using Horovod
# MPI Implementation: OpenMPI 4.1+
# GPU Support: NVIDIA GPUs via nvidia.com/gpu resource
#
# Example Usage:
#   kubectl apply -f mpi-horovod-gpu.yaml
#   # Then create a TrainJob referencing this runtime

apiVersion: kubeflow.org/v2alpha1
kind: ClusterTrainingRuntime
metadata:
  name: mpi-horovod-gpu
  labels:
    app.kubernetes.io/name: trainingruntime
    app.kubernetes.io/component: mpi
    app.kubernetes.io/managed-by: training-operator
    training.kubeflow.org/framework: mpi
    training.kubeflow.org/implementation: horovod
spec:
  # ML-specific configuration
  mlPolicy:
    # Default to single node (users override via TrainJob.spec.trainer.numNodes)
    numNodes: 1

    # MPI implementation variant
    mpiImplementation: OpenMPI

    # MPI processes per worker pod
    # For GPU training, typically 1 slot = 1 GPU
    slotsPerWorker: 1

  # Gang scheduling configuration
  podGroupPolicy:
    coscheduling:
      # Timeout for gang scheduling (5 minutes)
      # All pods (launcher + workers) must be schedulable within this time
      scheduleTimeoutSeconds: 300

    # Optional: High priority for training workloads
    # priorityClassName: training-high-priority

  # JobSet template for launcher and worker pods
  template:
    spec:
      replicatedJobs:
        # Launcher pod configuration
        - name: launcher
          replicas: 1
          template:
            metadata:
              labels:
                training.kubeflow.org/job-role: launcher
                training.kubeflow.org/replica-type: launcher
              annotations:
                # Disable Istio sidecar injection for training pods
                sidecar.istio.io/inject: "false"
            spec:
              # Service account for launcher (needs RBAC for worker discovery)
              serviceAccountName: training-operator

              # Restart policy for launcher
              restartPolicy: OnFailure

              containers:
                - name: launcher
                  # Image will be overridden by TrainJob.spec.trainer.image
                  image: horovod/horovod:latest

                  # Command will be injected by controller
                  # Controller generates: mpirun --hostfile /etc/mpi/hostfile ...
                  command:
                    - mpirun
                    - --allow-run-as-root
                    - --hostfile
                    - /etc/mpi/hostfile
                    - --mca
                    - btl_tcp_if_exclude
                    - lo,docker0
                    - --bind-to
                    - none
                    - --map-by
                    - slot
                    - -x
                    - NCCL_DEBUG=INFO
                    - -x
                    - NCCL_SOCKET_IFNAME=^docker0,lo
                    - -x
                    - LD_LIBRARY_PATH
                    - -x
                    - PATH

                  # Resource requests (controller may override based on TrainJob)
                  resources:
                    requests:
                      cpu: "2"
                      memory: "4Gi"
                    limits:
                      cpu: "4"
                      memory: "8Gi"

                  # Volume mounts for SSH keys and hostfile
                  volumeMounts:
                    - name: ssh-auth
                      mountPath: /root/.ssh
                      readOnly: true
                    - name: hostfile
                      mountPath: /etc/mpi
                      readOnly: true

                  # Environment variables
                  env:
                    - name: OMPI_ALLOW_RUN_AS_ROOT
                      value: "1"
                    - name: OMPI_ALLOW_RUN_AS_ROOT_CONFIRM
                      value: "1"

              # Volumes (populated by controller)
              volumes:
                - name: ssh-auth
                  secret:
                    secretName: $(JOB_NAME)-ssh
                    defaultMode: 0600
                    items:
                      - key: ssh-privatekey
                        path: id_rsa
                      - key: ssh-publickey
                        path: authorized_keys
                      - key: config
                        path: config
                - name: hostfile
                  configMap:
                    name: $(JOB_NAME)-hostfile
                    items:
                      - key: hostfile
                        path: hostfile

        # Worker pod configuration
        - name: worker
          # Replicas will be set by controller based on TrainJob.spec.trainer.numNodes
          replicas: 1
          template:
            metadata:
              labels:
                training.kubeflow.org/job-role: worker
                training.kubeflow.org/replica-type: worker
              annotations:
                sidecar.istio.io/inject: "false"
            spec:
              restartPolicy: OnFailure

              containers:
                - name: worker
                  # Image will be overridden by TrainJob.spec.trainer.image
                  image: horovod/horovod:latest

                  # Workers run sshd to accept connections from launcher
                  command:
                    - /bin/bash
                    - -c
                    - |
                      set -e

                      # Start SSH daemon
                      mkdir -p /var/run/sshd
                      /usr/sbin/sshd -D -p 2222 &
                      SSHD_PID=$!

                      # Wait for training to complete
                      # SSH daemon will be killed when pod terminates
                      wait $SSHD_PID || true

                      # Keep container running until training completes
                      sleep infinity

                  # Ports (SSH on port 2222 to avoid requiring privileged SCC)
                  ports:
                    - name: ssh
                      containerPort: 2222
                      protocol: TCP

                  # Resource requests (will be overridden by TrainJob.spec.trainer.resourcesPerNode)
                  resources:
                    requests:
                      cpu: "4"
                      memory: "16Gi"
                      nvidia.com/gpu: "1"
                    limits:
                      cpu: "8"
                      memory: "32Gi"
                      nvidia.com/gpu: "1"

                  # Volume mounts
                  volumeMounts:
                    - name: ssh-auth
                      mountPath: /root/.ssh
                      readOnly: true
                    - name: sshd-config
                      mountPath: /etc/ssh/sshd_config
                      subPath: sshd_config
                      readOnly: true

                  # Environment variables
                  env:
                    - name: OMPI_ALLOW_RUN_AS_ROOT
                      value: "1"
                    - name: OMPI_ALLOW_RUN_AS_ROOT_CONFIRM
                      value: "1"
                    # GPU-specific environment
                    - name: NVIDIA_VISIBLE_DEVICES
                      value: all
                    - name: NVIDIA_DRIVER_CAPABILITIES
                      value: compute,utility

              # Volumes
              volumes:
                - name: ssh-auth
                  secret:
                    secretName: $(JOB_NAME)-ssh
                    defaultMode: 0600
                    items:
                      - key: ssh-privatekey
                        path: id_rsa
                      - key: ssh-publickey
                        path: authorized_keys
                - name: sshd-config
                  configMap:
                    name: $(JOB_NAME)-sshd-config
                    items:
                      - key: sshd_config
                        path: sshd_config

---
# ConfigMap for sshd configuration
# This is a template - controller creates per-job instance
apiVersion: v1
kind: ConfigMap
metadata:
  name: training-sshd-config-template
data:
  sshd_config: |
    # SSH Server Configuration for MPI Training Pods
    # Port 2222 (avoids requiring NET_BIND_SERVICE SCC)
    Port 2222

    # Authentication
    PubkeyAuthentication yes
    PasswordAuthentication no
    ChallengeResponseAuthentication no

    # Disable strict host key checking for ephemeral training pods
    StrictModes no

    # Allow root login (training pods run as root by default)
    PermitRootLogin yes

    # Authorized keys
    AuthorizedKeysFile /root/.ssh/authorized_keys

    # Logging
    LogLevel INFO

    # Performance
    UseDNS no

    # Disable unnecessary features
    X11Forwarding no
    AllowTcpForwarding yes
    PermitTunnel no
